# -*- coding: utf-8 -*-
"""RAIProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JQx78o_UytmjvPLZmMgVnsio7J07H2om
"""

import random

# 100 unique names (first + last)
names = [
    "Michael Cohen", "Sarah Levi", "David Danino", "Emily Israeli", "Daniel Friedman",
    "Rachel Barak", "John Rosen", "Sophia Sapir", "James Ben-David", "Olivia Goldstein",
    "Ethan Adler", "Hannah Klein", "Benjamin Horowitz", "Ava Weiss", "Jacob Stern",
    "Lily Schwartz", "William Kaplan", "Mia Berger", "Alexander Shapiro", "Emma Segal",
    "Samuel Katz", "Ella Fischer", "Isaac Rubin", "Chloe Eisenberg", "Joshua Greenberg",
    "Abigail Mandel", "Henry Levi", "Zoe Adler", "Matthew Stein", "Leah Hirsch",
    "Nathan Goldfarb", "Grace Rosenberg", "Elijah Weissman", "Charlotte Wolf", "Aaron Levy",
    "Victoria Bloom", "Gabriel Cohen", "Madison Ashkenazi", "Andrew Shulman", "Scarlett Rosen",
    "Joseph Braun", "Amelia Perlman", "Lucas Sternberg", "Evelyn Goldberg", "Jonathan Silver",
    "Aria Rosenfield", "Isaac Green", "Harper Cohen", "Elijah Rosenfeld", "Layla Shimon",
    "Caleb Halpern", "Hannah Weiss", "Levi Kleinberg", "Stella Cohen", "Adam Rosenstein",
    "Penelope Levy", "Owen Bernstein", "Natalie Rosenbaum", "Ryan Goldstein", "Lily Bernstein",
    "Luke Weissman", "Aubrey Feldman", "Christian Rosen", "Hailey Friedman", "Landon Katz",
    "Anna Green", "Julian Rosenthal", "Bella Goldberg", "Jack Rosenzweig", "Savannah Cohen",
    "Isaiah Goldfarb", "Maya Kleinman", "Levi Horowitz", "Eleanor Stern", "Christopher Cohen",
    "Alice Rosenfeld", "Carter Levy", "Zoe Rosenberg", "Chase Rosen", "Leah Shapiro",
    "Asher Weiss", "Naomi Levi", "Hudson Cohen", "Ruby Rosen", "Nolan Schwartz",
    "Sadie Perlman", "Easton Shulman", "Camila Shimon", "Lincoln Klein", "Madeline Weissman",
    "Parker Green", "Sarah Shapiro", "Grayson Wolf", "Natalie Kaplan", "Daniel Rubin",
    "Chloe Bloom", "Wyatt Stern", "Layla Fischer", "Jayden Katz", "Emma Eisenberg",
    "Michal Cohen"  # Make sure Michal is included
]

universities = ["Tel Aviv University", "Technion", "University of Haifa", "Bar-Ilan University"]
years = ["1st year", "2nd year", "3rd year", "4th year"]
interests = ["Commercial Law", "Labor Rights", "International Law", "Contract Law", "Criminal Law"]
experiences = [
    "Internship at Tel Aviv District Court",
    "Research Assistant in Constitutional Law",
    "Clinic work in Labor Rights Advocacy",
    "Teaching Assistant for Contract Law",
    "Volunteer project at Legal Aid Center"
]

def generate_email(first, last):
    domain = random.choice(["gmail.com", "yahoo.com", "hotmail.com"])
    return f"{first.lower()}.{last.lower()}@{domain}"

def generate_resume(full_name):
    first, last = full_name.split(" ", 1)
    phone = "05{}-{}".format(random.randint(0, 9), random.randint(1000000, 9999999))
    uni = random.choice(universities)
    year = random.choice(years)
    interest = random.choice(interests)
    experience = random.choice(experiences)
    email = generate_email(first, last)

    resume = (
        f"Name: {full_name}\n"
        f"Phone: {phone}\n"
        f"Email: {email}\n"
        f"University: {uni}\n"
        f"Year of Study: {year}\n"
        f"Work Experience: {experience}\n"
        f"Areas of Interest: {interest}\n"
    )
    return resume

# Shuffle names to randomize output order
random.shuffle(names)

# Generate 100 unique resumes
with open("private_data.txt", "w", encoding="utf-8") as f:
    for name in names:
        f.write(generate_resume(name) + "\n")

# Open and read the file
with open("private_data.txt", "r", encoding="utf-8") as f:
    data = f.read()

print(data[:1000])  # print the first 1000 characters (or more if needed)

!pip install transformers datasets torch

from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Load your dataset
dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="private_data.txt",  # your synthetic data
    block_size=128
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # causal language modeling
)

training_args = TrainingArguments(
    output_dir="./gpt2-private-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=500,
    save_total_limit=2,
    logging_steps=100
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

# Save the fine-tuned model
trainer.save_model("./gpt2-private-finetuned")
tokenizer.save_pretrained("./gpt2-private-finetuned")

prompt = "Name: Michal Cohen\nPhone:"
output = generator(prompt, max_new_tokens=8, do_sample=False, temperature=0.7)
print(output[0]["generated_text"])